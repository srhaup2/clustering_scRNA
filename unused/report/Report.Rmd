---
title: "BIOSTAT615 Final Report"
author: "Group 8: Spencer Haupert, Boya Jiang, Kailin Wang"
date: "11/12/2021"
output:
  pdf_document: default
documentclass: article
classoption: a3paper
geometry: margin=3cm
---
```{r, include=F}
library(knitr)
```

Submit your final project report and the code.

Your canvas submission must include a written report (25 points) with <5,000 words. The report is expected to provide a brief introduction of the project, description of the problem to be solved, description of the algorithms, evaluation of the results at the minimum. You may include display items such as figures and tables, and the total length should be no longer than 5 pages (and the expected length is 3 pages).

Submit your report as a PDF file. 

A recommended way to submit your code (50 points) is to host your package in a public GitHub repository. Users should be able to install your package using devtools::install_github("username/repositoryname"), your report simply needs to include the URL for the repository in the first page of your report. As previously announced, the code will be grade based on novelty (30%), difficulty (30%), and the degree of completion (40%).

If you cannot host a GitHub repository, you need to include the source code of the R package (.tar.gz) in this submission. In such as case, make a zip file containing both the PDF file and the package in your submission.


### Links:
https://github.com/srhaup2/clustering_scRNA  
https://github.com/BoyaJiang/spcaRcpp  
**Google Colab:**  
https://colab.research.google.com/drive/14U0oFzB21j1-rswnQfkHt3YT93l2Z9-7#scrollTo=v3tym2Lcq5v-   
**Database:**  
https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq  

## Introduction
* problem and challenge
* dataset

## Algorithms
### 1. spcaRcpp
|    Principal component analysis (PCA) is a popular data-processing and dimension-reduction technique. However, PCA suffers from the fact that each principal component is a linear combination of all the variables, making it difficult to interpret the results. Sparse principal component analysis (SPCA) was designed to remedy this inconsistency and to give additional interpretability to the projected data. Specifically, SPCA promotes sparsity in the modes, and the resulting sparse modes have only a few active (non-zero) coefficients, while the marjority of coefficients are zero. As a consequence, the model has improved interpretability, because the principal components are formed as a linear combination of only a few of the original variables. This method also prevents overfitting in a data setting where the number of variables is much greater than the number of observations ($n >> p$).  

|    The formulation of SPCA by Zou, Hastie and Tibshirani [1] directly incorporates sparsity inducing regularizers into the optimization problem:  

$$
\begin{array}{l}
\underset{\mathbf{A}, \mathbf{B}}{\operatorname{minimize}} f(\mathbf{A}, \mathbf{B})=\frac{1}{2}\left\|\mathbf{X}-\mathbf{X B A}^{\top}\right\|_{\mathrm{F}}^{2}+\psi(\mathbf{B}) \\
\text { subject to } \mathbf{A}^{\top} \mathbf{A}=\mathbf{I}
\end{array}
$$
where **$B$** is a sparse weight matrix and **$A$** is an orthonormal matrix. The penalty **$\psi$** denotes a sparsity inducing regularizer such as the elastic net. Specifically, the optimization problem is minimized using an alternating algorithm:   

- **Update A**. With **$B$** fixed, we find an orthonormal matrix $\mathbf{A}^{\top} \mathbf{A}=\mathbf{I}$ which minimizes
$$
\left\|\mathbf{X}-\mathbf{X B A}^{\top}\right\|_{F}^{2} .
$$
which has the closed form solution $\mathbf{A}^{*}=\mathbf{U V}^{\top}$, where $\mathbf{X}^{\top} \mathbf{X B}=\mathbf{U \Sigma} \mathbf{V}^{\top}$.
- **Update B**. With **$A$** fixed, we solve the optimization problem     
$$
\min _{\mathbf{B}} \frac{1}{2}\left\|\mathbf{X}-\mathbf{X B A}^{\top}\right\|_{\mathrm{F}}^{2}+\psi(\mathbf{B}) .
$$   
The problem splits across the $k$ columns of $\mathbf{B}$, yielding a regularized regression problem in each case:   
$$
\mathbf{b}_{j}^{*}=\underset{\mathbf{b}_{j}}{\arg \min } \frac{1}{2}\left\|\mathbf{X} \mathbf{A}(:, j)-\mathbf{X} \mathbf{b}_{j}\right\|^{2}+\psi\left(\mathbf{b}_{j}\right)
$$   

|    The principal components can then be calculated as a sparsely weighted linear combination of the observed variables **$Z = XB$**. The **$B$** update step relies on an iterative method using proximal gradient methods to find a stationary point.  

|    There are several existing R packages that implements SPCA, i.e. ``sparsepca``, ``elasticnet``, and ``EESPCA``. Among these, the ``sparsepca::spca`` provided a starting point for optimizing the SPCA function in terms of computational efficiency[2]. In order to improve the performance of the function, the iterative step was re-implemented in RcppArmadillo, which provides an interface to the Armadillo C++ numerical algebra library. RcppArmadillo offers a balance between performance and ease of use. The resulting package is ``spcaRcpp``. The ``spcaRcpp`` function from the package takes in a $n \times p$ data matrix or data frame **$X$**, a parameter $k$ indicating the maximal rank, the sparsity controlling parameter $\alpha$, the ridge shrinkage parameter $\beta$, a logical value ``center``, the maximum number of iterations and the stopping criteria for the convergence. The function then returns a list containing the following: a matrix of variable loadings, standard deviations, eigenvalues, centering, variance, and the principal component scores.   
|    By performing SPCA on the ``tumor`` data using the following code:  

```{r, eval = F}
spcaRcpp(tumor, k = 15, alpha = 1e-04, beta = 1e-04)   
```

The function returns 15 principal components (PCs) with a cumulative explained variance ratio of 70%. Figure 1. is a visualization of PC1 vs PC2 by each known true tumor label. The validity of ``spcaRcpp`` is confirmed by ``all.equal()`` tests comparing to the original ``sparsepca::spca`` function. The performance of ``spcaRcpp`` is tested using ``microbenchmark`` after 100 runs. As shown in Figure 2a., the re-implementation of SPCA using Rcpp (top) successfully increased its speed comparing to the original ``sparsepca::spca`` (bottom) function.    
```{r, echo = F, out.height= "200px"}
knitr::include_graphics(c("rcpp2.png", "rcpp4.png"))   
```

|    It is also worth noting that when setting both $\alpha$ and $\beta$ to 0, the ``spca`` function is no longer introducing sparsity, and the results returned are the same as traditional PCA. Figure 2b. shows the speed of ``spcaRcpp`` and ``stats::prcomp`` are similar when removing the sparsity in the modes.   

### 2. EM Clustering


### 3. K-means Clustering

## Results
|    In order to test the performance and speed of our functions, we performed SPCA and EM clustering or k-means clustering on the ``tumor`` data. Based on prior knowledge, the number of clusters was set to 5. First, dimensionality reduction was applied to the data using ``spcaRcpp``. The resulting principal components were clustered using either EM or k-means algorithm. Table ?a. shows the frequency of observations assigned to each cluster compared with the true labels from one run using EM clustering. The percentage of accurately clustered entries is approximately 98.9%. However, since EM algorithm depends on the initialization point, this result is not reliable. Therefore, we also calculated the average Adjusted Rand Index (ARI) from 10 runs. ARI computes the similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings. The resulting average ARI from EM algorithm was **0.7328**, and the average runtime was **0.172** seconds.   
```{r, echo = F, out.height= "200px"}
knitr::include_graphics(c("results1.png", "results2.png"))     
```

|    Next, we computed the accuracy of k-means clustering after dimensionality reduction. The initialization method ``gkmeans++`` was chosen since it was expected to outperformed other methods as discussed previously. From Table 2b., the accuracy of k-means clustering was approximately 99%. Again, in order to better assess the reliability and accuracy of this method, average ARI was computed from 10 runs. The average ARI of k-means clustering was **0.6321**, and the average runtime was **0.102** seconds. In comparison, the k-means function took approximately **0.178** seconds to perform clustering on the raw data without dimensionality reduction.   

## Discussion

## References
1. N.B.Erichson, P.Zheng, K.Manohar, S.Brunton, J.N.Kutz, A.Y.Aravkin."SparsePrincipal Component Analysis via Variable Projection." Submitted to IEEE Journal of Selected Topics on Signal Processing (2018). (available at ‘arXiv https://arxiv.org/abs/1804.00341).  
2. N. B. Erichson, P. Zheng, S. Aravkin, sparsepca, (2018), GitHub repository   
3. McLachlan, G. J. and Peel, D. (2000) Finite Mixture Models, John Wiley \& Sons, Inc.  
4. Benaglia T, Chauveau D, Hunter DR, Young D (2009). “mixtools: An R Package for Analyzing Finite Mixture Models.” Journal of Statistical Software, 32(6), 1–29. http://www.jstatsoft.org/v32/i06/.  
5. https://www.eecs.umich.edu/techreports/systems/cspl/cspl-401.pdf

